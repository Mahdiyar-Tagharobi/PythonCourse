{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T14:38:13.948989400Z",
     "start_time": "2026-01-21T14:38:13.154344Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:18.059804700Z",
     "start_time": "2026-01-21T14:41:18.006657100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_digits()\n",
    "x = dataset.data\n",
    "y = dataset.target\n",
    "y = np.eye(10)[y] # One hot\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "id": "a95f63ae296fe2bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1437, 64), (360, 64), (1437, 10), (360, 10))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:47.397670Z",
     "start_time": "2026-01-21T14:41:47.369084100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def root_mean_squired_error(y_gt, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_gt - y_pred)))\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    return -np.sum(y * np.log(y_pred + 1e-9))"
   ],
   "id": "8da3f48eab1a990d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:47.895438500Z",
     "start_time": "2026-01-21T14:41:47.866607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 80\n",
    "lr = .001\n",
    "\n",
    "d_in = x_train.shape[1]\n",
    "h2 = 128\n",
    "h1 = 32\n",
    "d_out = y_train.shape[1]\n"
   ],
   "id": "35acb171b6a80dfa",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:48.299151200Z",
     "start_time": "2026-01-21T14:41:48.271255700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w1 = np.random.randn(d_in, h1)\n",
    "w2 = np.random.randn(h1, h2)\n",
    "w3 = np.random.randn(h2, d_out)"
   ],
   "id": "ec6ba62cc346b245",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:48.772992500Z",
     "start_time": "2026-01-21T14:41:48.758872400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "b1 = np.random.randn(1, h1)\n",
    "b2 = np.random.randn(1, h2)\n",
    "b3 = np.random.randn(1, d_out)"
   ],
   "id": "980321b4263e41f4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:41:57.465062300Z",
     "start_time": "2026-01-21T14:41:49.251699500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epochs in range(epochs):\n",
    "    # train\n",
    "    Y_pred_train = []\n",
    "    for x, y in zip(x_train, y_train):\n",
    "\n",
    "        x = x.reshape(1, -1)\n",
    "        y = y.reshape(1, -1)\n",
    "        # Forward\n",
    "\n",
    "        # layer 1\n",
    "        out1 = sigmoid(x @ w1 + b1)\n",
    "\n",
    "        # layer 2\n",
    "        out2 = sigmoid(out1 @ w2 + b2)\n",
    "\n",
    "        # Layer 3 or predicted \"y\", because this layers is output layer\n",
    "        # In classification situations, we usually use softmax activation function\n",
    "        y_pred = softmax(out2 @ w3 + b3)\n",
    "\n",
    "        # loss = cross_entropy(y, y_pred)\n",
    "        error = y_pred - y\n",
    "        Y_pred_train.append(y_pred)\n",
    "\n",
    "        # backward: you must calculate derivative for each layer many times\n",
    "        # layer 3\n",
    "        error = y - y_pred\n",
    "        grad_b3 = error\n",
    "        grad_w3 = out2.T @ error\n",
    "\n",
    "\n",
    "        # layer 2\n",
    "        error = error @ w3.T * out2 * (1 - out2)\n",
    "        grad_b2 = error\n",
    "        grad_w2 = out1.T @ error\n",
    "\n",
    "        # layer 1\n",
    "        error = error @ w2.T * out1 * (1 - out1)\n",
    "        grad_b1 = error\n",
    "        grad_w1 = x.T @ error\n",
    "\n",
    "        # update\n",
    "\n",
    "        # layer 1\n",
    "        w1 = w1 - (lr * grad_w1)\n",
    "        b1 = b1 - (lr * grad_b1)\n",
    "\n",
    "        # layer 2\n",
    "        w2 = w2 - lr * grad_w2\n",
    "        b2 = b2 - lr * grad_b2\n",
    "\n",
    "        # layer 3\n",
    "        w3 = w3 - lr * grad_w3\n",
    "        b3 = b3 - lr * grad_b3\n",
    "\n",
    "        # acc\n",
    "\n",
    "        # evaluate\n",
    "\n",
    "        acc = ...\n",
    "\n",
    "    # test\n",
    "    Y_pred_test = []\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        # Forward\n",
    "\n",
    "        # layer 1\n",
    "        out1 = sigmoid(x @ w1 + b1)\n",
    "\n",
    "        # layer 2\n",
    "        out2 = sigmoid(out1 @ w2 + b2)\n",
    "\n",
    "        # Layer 3 or predicted \"y\", because this layers is output layer\n",
    "        # In classification situations, we usually use softmax activation function\n",
    "        y_pred = softmax(out2 @ w3 + b3)\n",
    "\n",
    "        # loss = cross_entropy(y, y_pred)\n",
    "        error = y_pred - y\n",
    "        Y_pred_test.append(y_pred)\n",
    "\n",
    "    # train results\n",
    "    Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n",
    "    loss_train = root_mean_squired_error(Y_pred_train, y_train)\n",
    "    accuracy_train = np.sum(np.argmax(y_train, axis=1) == np.argmax(Y_pred_train , axis=1)) / len(y_train)\n",
    "    print(f\"loss_train: {loss_train}\")\n",
    "    print(f\"accuracy_train: {accuracy_train}\")\n",
    "\n",
    "    # test results\n",
    "    Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n",
    "    loss_test = root_mean_squired_error(Y_pred_test, y_test)\n",
    "    accuracy_test = np.sum(np.argmax(y_test, axis=1) == np.argmax(Y_pred_test, axis=1)) / len(y_test)\n",
    "    print(f\"loss_test: {loss_test}\")\n",
    "    print(f\"accuracy_test: {accuracy_test}\")"
   ],
   "id": "26f5306f87401b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train: 0.34517048689459545\n",
      "accuracy_train: 0.13987473903966596\n",
      "loss_test: 0.309110103767163\n",
      "accuracy_test: 0.24444444444444444\n",
      "loss_train: 0.29491642242790966\n",
      "accuracy_train: 0.3173277661795407\n",
      "loss_test: 0.2844687700035382\n",
      "accuracy_test: 0.3472222222222222\n",
      "loss_train: 0.2713896831811188\n",
      "accuracy_train: 0.4279749478079332\n",
      "loss_test: 0.2656854160245652\n",
      "accuracy_test: 0.4444444444444444\n",
      "loss_train: 0.25331519651975937\n",
      "accuracy_train: 0.534446764091858\n",
      "loss_test: 0.25112237761885103\n",
      "accuracy_test: 0.55\n",
      "loss_train: 0.23832339709086092\n",
      "accuracy_train: 0.5901183020180932\n",
      "loss_test: 0.2393051508511419\n",
      "accuracy_test: 0.5916666666666667\n",
      "loss_train: 0.22536097141312922\n",
      "accuracy_train: 0.639526791927627\n",
      "loss_test: 0.2289058641454882\n",
      "accuracy_test: 0.6277777777777778\n",
      "loss_train: 0.2147813481684948\n",
      "accuracy_train: 0.6840640222686152\n",
      "loss_test: 0.22175207637737207\n",
      "accuracy_test: 0.6527777777777778\n",
      "loss_train: 0.2064943782907799\n",
      "accuracy_train: 0.708420320111343\n",
      "loss_test: 0.2168297853919547\n",
      "accuracy_test: 0.6638888888888889\n",
      "loss_train: 0.1994578884168365\n",
      "accuracy_train: 0.7292971468336813\n",
      "loss_test: 0.21213593758121366\n",
      "accuracy_test: 0.6805555555555556\n",
      "loss_train: 0.19330634364202098\n",
      "accuracy_train: 0.7508698677800975\n",
      "loss_test: 0.2081462762896715\n",
      "accuracy_test: 0.6972222222222222\n",
      "loss_train: 0.18780342176687995\n",
      "accuracy_train: 0.7640918580375783\n",
      "loss_test: 0.2034975142806055\n",
      "accuracy_test: 0.7111111111111111\n",
      "loss_train: 0.18256891940556136\n",
      "accuracy_train: 0.7738343771746694\n",
      "loss_test: 0.19919228755620993\n",
      "accuracy_test: 0.7138888888888889\n",
      "loss_train: 0.1777326736901915\n",
      "accuracy_train: 0.7870563674321504\n",
      "loss_test: 0.19475824917781134\n",
      "accuracy_test: 0.7222222222222222\n",
      "loss_train: 0.17354688108890118\n",
      "accuracy_train: 0.7995824634655533\n",
      "loss_test: 0.19125745389747098\n",
      "accuracy_test: 0.7416666666666667\n",
      "loss_train: 0.16940712996472762\n",
      "accuracy_train: 0.8100208768267223\n",
      "loss_test: 0.18781501555905872\n",
      "accuracy_test: 0.7472222222222222\n",
      "loss_train: 0.16572257749804245\n",
      "accuracy_train: 0.8176757132915797\n",
      "loss_test: 0.18431579341144988\n",
      "accuracy_test: 0.7611111111111111\n",
      "loss_train: 0.1621686738417668\n",
      "accuracy_train: 0.8281141266527487\n",
      "loss_test: 0.1815373201160327\n",
      "accuracy_test: 0.7694444444444445\n",
      "loss_train: 0.15893154498382275\n",
      "accuracy_train: 0.8392484342379958\n",
      "loss_test: 0.17910169557551198\n",
      "accuracy_test: 0.7722222222222223\n",
      "loss_train: 0.15593288548820297\n",
      "accuracy_train: 0.8434237995824635\n",
      "loss_test: 0.17670364010234327\n",
      "accuracy_test: 0.7722222222222223\n",
      "loss_train: 0.153290641181029\n",
      "accuracy_train: 0.848295059151009\n",
      "loss_test: 0.1744174587631255\n",
      "accuracy_test: 0.775\n",
      "loss_train: 0.15063270307724688\n",
      "accuracy_train: 0.8510786360473208\n",
      "loss_test: 0.17231464219578724\n",
      "accuracy_test: 0.7861111111111111\n",
      "loss_train: 0.1482502997330637\n",
      "accuracy_train: 0.8566457898399443\n",
      "loss_test: 0.17069527995549294\n",
      "accuracy_test: 0.7861111111111111\n",
      "loss_train: 0.14593661397116311\n",
      "accuracy_train: 0.8608211551844119\n",
      "loss_test: 0.1691995391147845\n",
      "accuracy_test: 0.7861111111111111\n",
      "loss_train: 0.14388000682119018\n",
      "accuracy_train: 0.8629088378566457\n",
      "loss_test: 0.16772611091570236\n",
      "accuracy_test: 0.7972222222222223\n",
      "loss_train: 0.1415781618887056\n",
      "accuracy_train: 0.8684759916492694\n",
      "loss_test: 0.16622213602878064\n",
      "accuracy_test: 0.8027777777777778\n",
      "loss_train: 0.1397215029541045\n",
      "accuracy_train: 0.871955462769659\n",
      "loss_test: 0.16496337543959871\n",
      "accuracy_test: 0.8027777777777778\n",
      "loss_train: 0.1379926925796487\n",
      "accuracy_train: 0.8761308281141267\n",
      "loss_test: 0.16370465237643841\n",
      "accuracy_test: 0.8027777777777778\n",
      "loss_train: 0.136346276344749\n",
      "accuracy_train: 0.8775226165622826\n",
      "loss_test: 0.16250987404179051\n",
      "accuracy_test: 0.8083333333333333\n",
      "loss_train: 0.13483881322452196\n",
      "accuracy_train: 0.8816979819067502\n",
      "loss_test: 0.16139348364299358\n",
      "accuracy_test: 0.8083333333333333\n",
      "loss_train: 0.13340407880493907\n",
      "accuracy_train: 0.8851774530271399\n",
      "loss_test: 0.16037775182535127\n",
      "accuracy_test: 0.8083333333333333\n",
      "loss_train: 0.13201440590550523\n",
      "accuracy_train: 0.8858733472512178\n",
      "loss_test: 0.15941206992555118\n",
      "accuracy_test: 0.8111111111111111\n",
      "loss_train: 0.13067829720060867\n",
      "accuracy_train: 0.8879610299234516\n",
      "loss_test: 0.1584366012428438\n",
      "accuracy_test: 0.825\n",
      "loss_train: 0.12935579901616942\n",
      "accuracy_train: 0.8914405010438413\n",
      "loss_test: 0.1574398969047538\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12811454945492862\n",
      "accuracy_train: 0.8914405010438413\n",
      "loss_test: 0.15642303742947933\n",
      "accuracy_test: 0.8361111111111111\n",
      "loss_train: 0.12698770498456458\n",
      "accuracy_train: 0.8963117606123869\n",
      "loss_test: 0.15534683105503397\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12585712591935175\n",
      "accuracy_train: 0.8983994432846207\n",
      "loss_test: 0.1543818487862359\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12472580956470246\n",
      "accuracy_train: 0.8983994432846207\n",
      "loss_test: 0.15356312352799673\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12375862470096811\n",
      "accuracy_train: 0.8983994432846207\n",
      "loss_test: 0.15286002239642543\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12283078052176172\n",
      "accuracy_train: 0.8983994432846207\n",
      "loss_test: 0.1521948056234612\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12189075283163295\n",
      "accuracy_train: 0.8990953375086986\n",
      "loss_test: 0.15158787913713925\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.12091045179451744\n",
      "accuracy_train: 0.9004871259568545\n",
      "loss_test: 0.15102172073247164\n",
      "accuracy_test: 0.8333333333333334\n",
      "loss_train: 0.11992828184174358\n",
      "accuracy_train: 0.9032707028531664\n",
      "loss_test: 0.1504900329174972\n",
      "accuracy_test: 0.8361111111111111\n",
      "loss_train: 0.11904763298852265\n",
      "accuracy_train: 0.9032707028531664\n",
      "loss_test: 0.15001082277383185\n",
      "accuracy_test: 0.8388888888888889\n",
      "loss_train: 0.11815767275429465\n",
      "accuracy_train: 0.9039665970772442\n",
      "loss_test: 0.14954325528022008\n",
      "accuracy_test: 0.8388888888888889\n",
      "loss_train: 0.11720600186431168\n",
      "accuracy_train: 0.906054279749478\n",
      "loss_test: 0.1491192254391891\n",
      "accuracy_test: 0.8388888888888889\n",
      "loss_train: 0.11627765786066994\n",
      "accuracy_train: 0.9088378566457899\n",
      "loss_test: 0.14877596108939148\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.11536124873903404\n",
      "accuracy_train: 0.9102296450939458\n",
      "loss_test: 0.14847542955926185\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.11446805719499353\n",
      "accuracy_train: 0.9123173277661796\n",
      "loss_test: 0.14816902763926398\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.1136194086092794\n",
      "accuracy_train: 0.9144050104384134\n",
      "loss_test: 0.14783809031375011\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.11280398100538797\n",
      "accuracy_train: 0.9144050104384134\n",
      "loss_test: 0.14752150331651878\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.11199184560466598\n",
      "accuracy_train: 0.9144050104384134\n",
      "loss_test: 0.14728356485742358\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.11113953359858919\n",
      "accuracy_train: 0.9144050104384134\n",
      "loss_test: 0.14711383566126132\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.11017751070187982\n",
      "accuracy_train: 0.9164926931106472\n",
      "loss_test: 0.1469452537338907\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.10906090312194525\n",
      "accuracy_train: 0.9199721642310369\n",
      "loss_test: 0.14651570422130833\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.10807953798294567\n",
      "accuracy_train: 0.9213639526791928\n",
      "loss_test: 0.14604709066671664\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.10724582911024845\n",
      "accuracy_train: 0.9220598469032707\n",
      "loss_test: 0.14567121283382478\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.10649779172912151\n",
      "accuracy_train: 0.9234516353514266\n",
      "loss_test: 0.1453460620828505\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.1057865564125852\n",
      "accuracy_train: 0.9248434237995825\n",
      "loss_test: 0.14506127633693436\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.1051079297776082\n",
      "accuracy_train: 0.9262352122477383\n",
      "loss_test: 0.14480040135157746\n",
      "accuracy_test: 0.8416666666666667\n",
      "loss_train: 0.10445518868348319\n",
      "accuracy_train: 0.9269311064718163\n",
      "loss_test: 0.14454711985434676\n",
      "accuracy_test: 0.8444444444444444\n",
      "loss_train: 0.10381060274931163\n",
      "accuracy_train: 0.9276270006958942\n",
      "loss_test: 0.14427333697340292\n",
      "accuracy_test: 0.8472222222222222\n",
      "loss_train: 0.10312795775262652\n",
      "accuracy_train: 0.9283228949199722\n",
      "loss_test: 0.1439309029743336\n",
      "accuracy_test: 0.8555555555555555\n",
      "loss_train: 0.10228519640359199\n",
      "accuracy_train: 0.930410577592206\n",
      "loss_test: 0.14354937110013888\n",
      "accuracy_test: 0.8527777777777777\n",
      "loss_train: 0.10158301965463973\n",
      "accuracy_train: 0.9324982602644398\n",
      "loss_test: 0.14326753783544777\n",
      "accuracy_test: 0.85\n",
      "loss_train: 0.10099869884065588\n",
      "accuracy_train: 0.9338900487125957\n",
      "loss_test: 0.14302893883988196\n",
      "accuracy_test: 0.85\n",
      "loss_train: 0.1004419180881332\n",
      "accuracy_train: 0.9345859429366736\n",
      "loss_test: 0.14277189831008189\n",
      "accuracy_test: 0.85\n",
      "loss_train: 0.09990156587051541\n",
      "accuracy_train: 0.9345859429366736\n",
      "loss_test: 0.1424904862124903\n",
      "accuracy_test: 0.8527777777777777\n",
      "loss_train: 0.09938623582068423\n",
      "accuracy_train: 0.9352818371607515\n",
      "loss_test: 0.14219331979741348\n",
      "accuracy_test: 0.8583333333333333\n",
      "loss_train: 0.09890396266937462\n",
      "accuracy_train: 0.9366736256089074\n",
      "loss_test: 0.14189336961799043\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.09845245935006386\n",
      "accuracy_train: 0.9366736256089074\n",
      "loss_test: 0.1416010262238974\n",
      "accuracy_test: 0.8638888888888889\n",
      "loss_train: 0.09802218310986027\n",
      "accuracy_train: 0.9366736256089074\n",
      "loss_test: 0.14132417408948855\n",
      "accuracy_test: 0.8638888888888889\n",
      "loss_train: 0.09760096134755071\n",
      "accuracy_train: 0.9373695198329853\n",
      "loss_test: 0.14106939245454805\n",
      "accuracy_test: 0.8666666666666667\n",
      "loss_train: 0.09717473916176911\n",
      "accuracy_train: 0.9373695198329853\n",
      "loss_test: 0.14084132729569734\n",
      "accuracy_test: 0.8666666666666667\n",
      "loss_train: 0.0967323299388154\n",
      "accuracy_train: 0.9380654140570633\n",
      "loss_test: 0.14064226899087326\n",
      "accuracy_test: 0.8666666666666667\n",
      "loss_train: 0.09626528947022599\n",
      "accuracy_train: 0.9366736256089074\n",
      "loss_test: 0.14047538344463337\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.09571859713059842\n",
      "accuracy_train: 0.9373695198329853\n",
      "loss_test: 0.140334272022382\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.09507970540949486\n",
      "accuracy_train: 0.9380654140570633\n",
      "loss_test: 0.14018127830720314\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.0943351890764596\n",
      "accuracy_train: 0.9387613082811412\n",
      "loss_test: 0.13997844272351792\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.09365887147972575\n",
      "accuracy_train: 0.942240779401531\n",
      "loss_test: 0.13976653652215673\n",
      "accuracy_test: 0.8611111111111112\n",
      "loss_train: 0.09310384104385566\n",
      "accuracy_train: 0.942936673625609\n",
      "loss_test: 0.1395188433744753\n",
      "accuracy_test: 0.8611111111111112\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:39:30.762342900Z",
     "start_time": "2026-01-21T14:39:30.679081600Z"
    }
   },
   "cell_type": "code",
   "source": "x",
   "id": "f375ac2f7ea3f2b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  2., 13., 13.,  1.,  0.,  0.,  0.,  0.,  9., 13.,  5.,\n",
       "        0.,  0.,  0.,  0.,  0., 13.,  5.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "       15.,  2.,  0.,  0.,  0.,  0.,  0.,  0., 15., 10.,  9.,  9.,  2.,\n",
       "        0.,  0.,  0., 16., 11.,  8., 11., 12.,  0.,  0.,  1., 14., 11.,\n",
       "        1.,  4., 13.,  0.,  0.,  0.,  3., 11., 16., 15.,  4.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:40:38.452989400Z",
     "start_time": "2026-01-21T14:40:38.402160Z"
    }
   },
   "cell_type": "code",
   "source": "print(x)",
   "id": "a470c86144ba602d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 2.]\n",
      " [13.]\n",
      " [13.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 9.]\n",
      " [13.]\n",
      " [ 5.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [13.]\n",
      " [ 5.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [15.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [15.]\n",
      " [10.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [ 2.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [16.]\n",
      " [11.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [12.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [14.]\n",
      " [11.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [13.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 3.]\n",
      " [11.]\n",
      " [16.]\n",
      " [15.]\n",
      " [ 4.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:42:00.142247200Z",
     "start_time": "2026-01-21T14:42:00.098778200Z"
    }
   },
   "cell_type": "code",
   "source": "x.reshape(1, -1)",
   "id": "8cc5682454838066",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  5., 11.,  0.,  0.,  0.,  0.,  0.,  1., 14.,  9.,\n",
       "         0.,  0.,  0.,  0.,  0.,  4., 14.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        10.,  8.,  0.,  0.,  0.,  0.,  0.,  0., 13.,  8.,  4.,  6.,  2.,\n",
       "         0.,  0.,  0., 11., 16., 13., 12., 13.,  0.,  0.,  0., 12., 14.,\n",
       "         4.,  5., 16.,  2.,  0.,  0.,  1.,  8., 16., 13.,  9.,  1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db78012bd9acf2a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
